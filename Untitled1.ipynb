{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7c698f",
   "metadata": {},
   "source": [
    "# Computer Vision & Food Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ef845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c17673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f54fb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25216 images belonging to 256 classes.\n",
      "Found 6179 images belonging to 256 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\patel\\anaconda3\\Anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5345fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569155c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FoodRecognitionModel:\n",
    "    def __init__(self):\n",
    "        self.food_recognition_model = None\n",
    "\n",
    "    def create_food_recognition_model(self, num_classes):\n",
    "        \"\"\"Define the food recognition model.\"\"\"\n",
    "        model = Sequential([\n",
    "            Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "            MaxPooling2D(2, 2),\n",
    "            Conv2D(64, (3, 3), activation='relu'),\n",
    "            MaxPooling2D(2, 2),\n",
    "            Conv2D(128, (3, 3), activation='relu'),\n",
    "            MaxPooling2D(2, 2),\n",
    "            Flatten(),\n",
    "            Dense(512, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def train_food_recognition_model(self, train_data_path, validation_data_path=None, num_classes=101, epochs=20):\n",
    "        \"\"\"Train the food recognition model.\"\"\"\n",
    "\n",
    "        # Create output folders if not present\n",
    "        os.makedirs('models', exist_ok=True)\n",
    "        os.makedirs('reports', exist_ok=True)\n",
    "\n",
    "        # Create the model\n",
    "        model = self.create_food_recognition_model(num_classes)\n",
    "\n",
    "        # Data augmentation\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            validation_split=0.2,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "        # Training data\n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            r'C:\\Users\\patel\\Downloads\\UECFOOD256',\n",
    "            target_size=(224, 224),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical',\n",
    "            subset='training'\n",
    "        )\n",
    "\n",
    "        # Validation data\n",
    "        validation_generator = train_datagen.flow_from_directory(\n",
    "            r'C:\\Users\\patel\\Downloads\\UECFOOD256',\n",
    "            target_size=(224, 224),\n",
    "            batch_size=32,\n",
    "            class_mode='categorical',\n",
    "            subset='validation'\n",
    "        )\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            train_generator,\n",
    "            steps_per_epoch=train_generator.samples // 32,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=validation_generator.samples // 32\n",
    "        )\n",
    "\n",
    "        # Save the model\n",
    "        model.save('models/food_recognition_model.h5')\n",
    "        self.food_recognition_model = model\n",
    "\n",
    "        # Plot training history\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "        plt.title('Model Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('reports/food_recognition_training.png')\n",
    "\n",
    "        return model, history\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def identify_food(self, image_path):\n",
    "        \"\"\"Identify food from an image\"\"\"\n",
    "        if self.food_recognition_model is None:\n",
    "            print(\"Food recognition model not loaded\")\n",
    "            return None\n",
    "        \n",
    "        img = image.load_img(image_path, target_size=(224, 224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
    "        \n",
    "        prediction = self.food_recognition_model.predict(img_array)\n",
    "        \n",
    "        # Get class labels\n",
    "        class_indices = {v: k for k, v in self.food_recognition_model.class_indices.items()}\n",
    "        predicted_class = class_indices[np.argmax(prediction)]\n",
    "        confidence = np.max(prediction)\n",
    "        \n",
    "        return {\n",
    "            'food_type': predicted_class,\n",
    "            'confidence': float(confidence)\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def predict_sodium_content(self, food_type):\n",
    "        \"\"\"Predict sodium content based on food type\"\"\"\n",
    "        if self.food_database is not None:\n",
    "            # Look up sodium content in database\n",
    "            if food_type in self.food_database['food_name'].values:\n",
    "                sodium_content = self.food_database[self.food_database['food_name'] == food_type]['sodium_content'].values[0]\n",
    "                return sodium_content\n",
    "        \n",
    "        if self.sodium_prediction_model is not None:\n",
    "            # If we have a ML model for sodium prediction, use it\n",
    "            # This would need features extracted from the food type or image\n",
    "            features = self._extract_food_features(food_type)\n",
    "            sodium_content = self.sodium_prediction_model.predict([features])[0]\n",
    "            return sodium_content\n",
    "            \n",
    "        # Default fallback\n",
    "        return None\n",
    "    \n",
    "    def _extract_food_features(self, food_type):\n",
    "        \"\"\"Extract features from food type for sodium prediction\"\"\"\n",
    "        # This is a placeholder - in a real implementation, you would have a database\n",
    "        # of food features or extract them from images\n",
    "        return [0.5, 0.5, 0.5]  # Placeholder features\n",
    "    \n",
    "    def recommend_stimulation_level(self, food_type, user_preference):\n",
    "        \"\"\"Recommend electrical stimulation level based on food and user preference\"\"\"\n",
    "        sodium_content = self.predict_sodium_content(food_type)\n",
    "        \n",
    "        if sodium_content is None:\n",
    "            # Default recommendation\n",
    "            return 5  # Mid-level stimulation\n",
    "        \n",
    "        # Logic for recommending stimulation level\n",
    "        # Higher sodium content foods need less stimulation\n",
    "        base_level = 10 - min(sodium_content / 100, 9)  # Scale from 1-10\n",
    "        \n",
    "        # Adjust based on user preference\n",
    "        # user_preference should be a value between 0-1 indicating preference for saltiness\n",
    "        adjusted_level = base_level * (1 + (user_preference - 0.5))\n",
    "        \n",
    "        # Ensure the level is between 1-10\n",
    "        return max(1, min(10, adjusted_level))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9032f1fc",
   "metadata": {},
   "source": [
    "# Data Collection & Analysis for Market Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9679d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_survey_data(self, survey_results_path):\n",
    "    \"\"\"Analyze survey data for market research\"\"\"\n",
    "    # Load survey data\n",
    "    survey_data = pd.read_csv('survey_data.csv')\n",
    "    \n",
    "    # Preprocess data: Create derived columns for analysis\n",
    "    # Convert text responses to numeric values for analysis\n",
    "    \n",
    "    # Map yes/no responses to binary\n",
    "    binary_map = {'Yes': 1, 'No': 0}\n",
    "    if 'Do you follow a low-sodium diet?' in survey_data.columns:\n",
    "        survey_data['sodium_restriction'] = survey_data['Do you follow a low-sodium diet?'].map(binary_map).fillna(0)\n",
    "    \n",
    "    # Map frequency to numeric values\n",
    "    frequency_map = {'Daily': 4, 'Weekly': 3, 'Monthly': 2, 'Rarely': 1}\n",
    "    if 'How often do you eat at restaurants?' in survey_data.columns:\n",
    "        survey_data['restaurant_frequency'] = survey_data['How often do you eat at restaurants?'].map(frequency_map).fillna(0)\n",
    "    \n",
    "    # Map technology comfort\n",
    "    tech_comfort_map = {'Yes': 1, 'No': 0}\n",
    "    if 'Are you aware of technologies that enhance taste perception without adding sodium?' in survey_data.columns:\n",
    "        survey_data['technology_comfort'] = survey_data['Are you aware of technologies that enhance taste perception without adding sodium?'].map(tech_comfort_map).fillna(0)\n",
    "    \n",
    "    # Map purchase interest\n",
    "    purchase_map = {'Yes': 2, 'Maybe': 1, 'No': 0}\n",
    "    if 'Would you consider purchasing such a device if it improves taste satisfaction without adding sodium?' in survey_data.columns:\n",
    "        survey_data['purchase_interest'] = survey_data['Would you consider purchasing such a device if it improves taste satisfaction without adding sodium?'].map(purchase_map).fillna(0)\n",
    "    \n",
    "    # Map importance level\n",
    "    importance_map = {'Very Important': 3, 'Moderately Important': 2, 'Not Important': 1}\n",
    "    if 'How important is taste enhancement in your dining experience?' in survey_data.columns:\n",
    "        survey_data['taste_importance'] = survey_data['How important is taste enhancement in your dining experience?'].map(importance_map).fillna(1)\n",
    "    \n",
    "    # Map salt preference\n",
    "    salt_preference_map = {'Too Salty': 2, 'Perfect': 1, 'Too Bland': 0}\n",
    "    if 'Do you find the current salt content in these dishes' in survey_data.columns:\n",
    "        survey_data['salt_preference'] = survey_data['Do you find the current salt content in these dishes'].map(salt_preference_map).fillna(1)\n",
    "    \n",
    "    # Map device interest\n",
    "    device_interest_map = {'Yes': 1, 'No': 0}\n",
    "    if 'Would you be interested in trying a device that enhances salty and umami flavors using electric stimulation?' in survey_data.columns:\n",
    "        survey_data['device_interest'] = survey_data['Would you be interested in trying a device that enhances salty and umami flavors using electric stimulation?'].map(device_interest_map).fillna(0)\n",
    "    \n",
    "    # Convert age to age groups\n",
    "    survey_data['age_group'] = pd.cut(survey_data['Age'], bins=[0, 25, 40, 60, 100], labels=['18-25', '26-40', '41-60', '60+'])\n",
    "    \n",
    "    # Calculate price sensitivity (proxy from purchase interest and importance)\n",
    "    survey_data['price_sensitivity'] = 3 - (survey_data['purchase_interest'] / 2)  # Higher value means more sensitive\n",
    "    \n",
    "    # Basic analysis by age group\n",
    "    demographic_summary = survey_data.groupby('age_group').agg({\n",
    "        'sodium_restriction': 'mean',\n",
    "        'technology_comfort': 'mean',\n",
    "        'price_sensitivity': 'mean',\n",
    "        'device_interest': 'mean',\n",
    "        'purchase_interest': 'mean',\n",
    "        'taste_importance': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(demographic_summary, annot=True, cmap='Blues', fmt='.2f')\n",
    "    plt.title('User Characteristics by Age Group')\n",
    "    plt.savefig('reports/demographic_analysis.png')\n",
    "    \n",
    "    # Medical condition analysis\n",
    "    medical_conditions = survey_data.groupby('If yes, is it due to a medical condition? ( Type no incase of no)').size()\n",
    "    medical_conditions = medical_conditions[medical_conditions.index != 'No']\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    medical_conditions.plot(kind='bar')\n",
    "    plt.title('Distribution of Medical Conditions Among Low-Sodium Diet Followers')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('reports/medical_conditions.png')\n",
    "    \n",
    "    # Potential market size calculation\n",
    "    total_respondents = len(survey_data)\n",
    "    interested_users = survey_data[survey_data['device_interest'] == 1].shape[0]\n",
    "    market_penetration = interested_users / total_respondents\n",
    "    \n",
    "    print(f\"Potential market penetration: {market_penetration:.2%}\")\n",
    "    \n",
    "    # Device feature preferences\n",
    "    feature_columns = 'What features would you expect in a taste-enhancement device? (Select all that apply)'\n",
    "    feature_preferences = {}\n",
    "    \n",
    "    if feature_columns in survey_data.columns:\n",
    "        features = survey_data[feature_columns].dropna()\n",
    "        \n",
    "        for response in features:\n",
    "            for feature in response.split(';'):\n",
    "                feature_preferences[feature] = feature_preferences.get(feature, 0) + 1\n",
    "    \n",
    "    # Plot feature preferences\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    features_df = pd.DataFrame.from_dict(feature_preferences, orient='index', columns=['Count'])\n",
    "    features_df.sort_values(by='Count', ascending=False).plot(kind='bar')\n",
    "    plt.title('Preferred Features for Taste Enhancement Device')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('reports/feature_preferences.png')\n",
    "    \n",
    "    # Correlations between key factors\n",
    "    correlations = survey_data[\n",
    "        ['sodium_restriction', 'technology_comfort', 'price_sensitivity', \n",
    "         'purchase_interest', 'device_interest', 'Age']\n",
    "    ].corr()\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(correlations, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Correlations Between User Characteristics')\n",
    "    plt.savefig('reports/correlations.png')\n",
    "    \n",
    "    # Salt preferences across different dishes\n",
    "    salt_cols = [\n",
    "        'Dal / Gojju / Palya', 'Sambar / Rasam / Curd', \n",
    "        'Biryani / Pulao / Rice bath', 'Curries (Vegetable/Chicken/Mutton)',\n",
    "        'Dosa/Idly/Chaat/Snacks', 'Dosa/Idly/Roti/Paratha/Chapathi',\n",
    "        'Pickles/Papad'\n",
    "    ]\n",
    "    \n",
    "    # Convert salt measures to numeric values\n",
    "    salt_map = {'No Salt': 0, '1/4 tsp': 0.25, '1/2 tsp': 0.5, '1 tsp': 1, 'More than 1 tsp': 1.5}\n",
    "    for col in salt_cols:\n",
    "        if col in survey_data.columns:\n",
    "            survey_data[f\"{col}_numeric\"] = survey_data[col].map(salt_map).fillna(0)\n",
    "    \n",
    "    numeric_salt_cols = [f\"{col}_numeric\" for col in salt_cols if f\"{col}_numeric\" in survey_data.columns]\n",
    "    \n",
    "    if numeric_salt_cols:\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        survey_data[numeric_salt_cols].mean().sort_values().plot(kind='bar')\n",
    "        plt.title('Average Salt Preference Across Different Dishes')\n",
    "        plt.ylabel('Average Salt Amount (tsp)')\n",
    "        plt.savefig('reports/salt_preferences.png')\n",
    "    \n",
    "    # Generate comprehensive report\n",
    "    report = {\n",
    "        'demographic_summary': demographic_summary.to_dict(),\n",
    "        'market_penetration': market_penetration,\n",
    "        'correlations': correlations.to_dict(),\n",
    "        'feature_preferences': feature_preferences,\n",
    "        'insights': {\n",
    "            'top_age_groups': demographic_summary.index[demographic_summary['purchase_interest'].nlargest(2).index].tolist(),\n",
    "            'price_sensitivity_correlation': correlations.loc['price_sensitivity', 'purchase_interest'],\n",
    "            'medical_condition_prevalence': (survey_data['If yes, is it due to a medical condition? ( Type no incase of no)'] != 'No').mean(),\n",
    "            'device_interest_by_sodium_restriction': survey_data.groupby('sodium_restriction')['device_interest'].mean().to_dict()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create a reports directory if it doesn't exist\n",
    "    os.makedirs('reports', exist_ok=True)\n",
    "    \n",
    "    with open('reports/market_research_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    return report\n",
    "\n",
    "# def analyze_usage_patterns(self, usage_logs_path):\n",
    "#     \"\"\"Analyze usage patterns from logs\"\"\"\n",
    "#     # Note: This function is kept as is since you mentioned to keep\n",
    "#     # any functions that don't need to be modified. If you have usage logs\n",
    "#     # in the future, this function will be useful.\n",
    "    \n",
    "#     # Load usage logs\n",
    "#     usage_data = pd.read_csv(usage_logs_path)\n",
    "    \n",
    "#     # Time series analysis\n",
    "#     usage_data['date'] = pd.to_datetime(usage_data['timestamp'])\n",
    "#     daily_usage = usage_data.groupby(usage_data['date'].dt.date).agg({\n",
    "#         'user_id': 'nunique',\n",
    "#         'session_duration': 'mean',\n",
    "#         'taste_intensity_level': 'mean'\n",
    "#     })\n",
    "    \n",
    "#     # Plot trends\n",
    "#     plt.figure(figsize=(14, 7))\n",
    "#     plt.subplot(2, 1, 1)\n",
    "#     plt.plot(daily_usage.index, daily_usage['user_id'], marker='o')\n",
    "#     plt.title('Daily Active Users')\n",
    "    \n",
    "#     plt.subplot(2, 1, 2)\n",
    "#     plt.plot(daily_usage.index, daily_usage['taste_intensity_level'], marker='o', color='green')\n",
    "#     plt.title('Average Taste Intensity Settings')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('reports/usage_trends.png')\n",
    "    \n",
    "#     # Identify patterns\n",
    "#     # User segments based on usage patterns\n",
    "#     usage_data['usage_frequency'] = usage_data.groupby('user_id')['user_id'].transform('count')\n",
    "#     usage_data['avg_session_duration'] = usage_data.groupby('user_id')['session_duration'].transform('mean')\n",
    "    \n",
    "#     user_segments = usage_data.groupby('user_id').agg({\n",
    "#         'usage_frequency': 'first',\n",
    "#         'avg_session_duration': 'first',\n",
    "#         'taste_intensity_level': 'mean'\n",
    "#     })\n",
    "    \n",
    "#     # K-means clustering to identify user segments\n",
    "#     from sklearn.cluster import KMeans\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "#     scaler = StandardScaler()\n",
    "#     scaled_features = scaler.fit_transform(user_segments)\n",
    "    \n",
    "#     kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "#     user_segments['cluster'] = kmeans.fit_predict(scaled_features)\n",
    "    \n",
    "#     # Visualize clusters\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     sns.scatterplot(\n",
    "#         x='usage_frequency', \n",
    "#         y='avg_session_duration', \n",
    "#         hue='cluster', \n",
    "#         size='taste_intensity_level',\n",
    "#         sizes=(50, 200),\n",
    "#         data=user_segments\n",
    "#     )\n",
    "#     plt.title('User Segments Based on Usage Patterns')\n",
    "#     plt.savefig('reports/user_segments.png')\n",
    "    \n",
    "#     # Generate insights\n",
    "#     cluster_profiles = user_segments.groupby('cluster').mean()\n",
    "    \n",
    "#     return {\n",
    "#         'daily_usage': daily_usage.to_dict(),\n",
    "#         'user_segments': cluster_profiles.to_dict(),\n",
    "#         'total_users': usage_data['user_id'].nunique(),\n",
    "#         'total_sessions': len(usage_data)\n",
    "#     }\n",
    "\n",
    "FoodRecognitionModel.analyze_survey_data = analyze_survey_data\n",
    "# FoodRecognitionModel.analyze_usage_patterns = analyze_usage_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0841a9",
   "metadata": {},
   "source": [
    "# Sentiment Analysis & User Behavior Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fab8516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99def196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e2320a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_user_feedback(self, feedback_data_path):\n",
    "    \"\"\"Analyze user feedback and sentiment\"\"\"\n",
    "    # For survey data, we'll use the feedback/concerns and suggestions from the survey\n",
    "    survey_data = pd.read_csv('survey_data.csv')\n",
    "    \n",
    "    # Extract feedback columns\n",
    "    feedback_columns = [\n",
    "        'Do you have any concerns regarding using technology for taste enhancement?',\n",
    "        'Any suggestions or feedback regarding low-sodium dining options?'\n",
    "    ]\n",
    "    \n",
    "    feedback_data = pd.DataFrame()\n",
    "    \n",
    "    # Combine feedback from different columns\n",
    "    for col in feedback_columns:\n",
    "        if col in survey_data.columns:\n",
    "            # Extract non-empty responses\n",
    "            valid_feedback = survey_data[col].dropna()\n",
    "            valid_feedback = valid_feedback[valid_feedback.str.strip() != '']\n",
    "            valid_feedback = valid_feedback[valid_feedback.str.lower() != 'no']\n",
    "            \n",
    "            if not valid_feedback.empty:\n",
    "                temp_df = pd.DataFrame({\n",
    "                    'feedback_text': valid_feedback,\n",
    "                    'feedback_type': col\n",
    "                })\n",
    "                feedback_data = pd.concat([feedback_data, temp_df])\n",
    "    \n",
    "    # If no valid feedback, create a sample to avoid errors\n",
    "    if feedback_data.empty:\n",
    "        feedback_data = pd.DataFrame({\n",
    "            'feedback_text': ['No feedback available'],\n",
    "            'feedback_type': ['None']\n",
    "        })\n",
    "    \n",
    "    # Initialize sentiment analyzer if not already done\n",
    "    if not hasattr(self, 'sentiment_analyzer'):\n",
    "        import nltk\n",
    "        from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "        nltk.download('vader_lexicon', quiet=True)\n",
    "        self.sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Basic sentiment analysis\n",
    "    feedback_data['sentiment'] = feedback_data['feedback_text'].apply(\n",
    "        lambda x: self.sentiment_analyzer.polarity_scores(x)['compound'] if isinstance(x, str) else 0\n",
    "    )\n",
    "    \n",
    "    # Categorize sentiment\n",
    "    feedback_data['sentiment_category'] = pd.cut(\n",
    "        feedback_data['sentiment'],\n",
    "        bins=[-1, -0.25, 0.25, 1],\n",
    "        labels=['Negative', 'Neutral', 'Positive']\n",
    "    )\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    sentiment_counts = feedback_data['sentiment_category'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(x='sentiment_category', data=feedback_data, palette='RdYlGn')\n",
    "    plt.title('Sentiment Distribution in User Feedback')\n",
    "    plt.savefig('reports/sentiment_distribution.png')\n",
    "    \n",
    "    # Extract common themes\n",
    "    if nltk.download('punkt', quiet=True) and nltk.download('stopwords', quiet=True):\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        from nltk.corpus import stopwords\n",
    "        from collections import Counter\n",
    "        \n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        def extract_keywords(text_series):\n",
    "            all_words = []\n",
    "            for text in text_series:\n",
    "                if isinstance(text, str):\n",
    "                    words = word_tokenize(text.lower())\n",
    "                    words = [w for w in words if w.isalpha() and w not in stop_words]\n",
    "                    all_words.extend(words)\n",
    "            return Counter(all_words).most_common(10)\n",
    "        \n",
    "        positive_feedback = feedback_data[feedback_data['sentiment_category'] == 'Positive']\n",
    "        negative_feedback = feedback_data[feedback_data['sentiment_category'] == 'Negative']\n",
    "        \n",
    "        positive_themes = extract_keywords(positive_feedback['feedback_text'])\n",
    "        negative_themes = extract_keywords(negative_feedback['feedback_text'])\n",
    "        \n",
    "        # Visualize themes\n",
    "        if positive_themes and negative_themes:\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            \n",
    "            if positive_themes:\n",
    "                plt.subplot(2, 1, 1)\n",
    "                sns.barplot(x=[item[1] for item in positive_themes], \n",
    "                            y=[item[0] for item in positive_themes],\n",
    "                            palette='Greens_r')\n",
    "                plt.title('Common Themes in Positive Feedback')\n",
    "                plt.xlabel('Frequency')\n",
    "            \n",
    "            if negative_themes:\n",
    "                plt.subplot(2, 1, 2)\n",
    "                sns.barplot(x=[item[1] for item in negative_themes], \n",
    "                            y=[item[0] for item in negative_themes],\n",
    "                            palette='Reds_r')\n",
    "                plt.title('Common Themes in Negative Feedback')\n",
    "                plt.xlabel('Frequency')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig('reports/feedback_themes.png')\n",
    "    \n",
    "    # Analyze concerns specifically\n",
    "    concern_column = 'Do you have any concerns regarding using technology for taste enhancement?'\n",
    "    if concern_column in survey_data.columns:\n",
    "        valid_concerns = survey_data[concern_column].dropna()\n",
    "        valid_concerns = valid_concerns[valid_concerns.str.strip() != '']\n",
    "        valid_concerns = valid_concerns[valid_concerns.str.lower() != 'no']\n",
    "        \n",
    "        if not valid_concerns.empty:\n",
    "            # Word cloud for concerns\n",
    "            try:\n",
    "                from wordcloud import WordCloud\n",
    "                \n",
    "                concerns_text = ' '.join(valid_concerns)\n",
    "                \n",
    "                wordcloud = WordCloud(width=800, height=400, \n",
    "                                     background_color='white', \n",
    "                                     max_words=100).generate(concerns_text)\n",
    "                \n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.imshow(wordcloud, interpolation='bilinear')\n",
    "                plt.axis('off')\n",
    "                plt.title('Concerns About Taste Enhancement Technology')\n",
    "                plt.savefig('reports/concerns_wordcloud.png')\n",
    "            except ImportError:\n",
    "                print(\"WordCloud package not available. Skipping word cloud visualization.\")\n",
    "    \n",
    "    # Generate report\n",
    "    report = {\n",
    "        'sentiment_distribution': sentiment_counts.to_dict(),\n",
    "        'average_sentiment': feedback_data['sentiment'].mean(),\n",
    "        'positive_themes': dict(positive_themes) if 'positive_themes' in locals() else {},\n",
    "        'negative_themes': dict(negative_themes) if 'negative_themes' in locals() else {},\n",
    "        'concerns_count': len(valid_concerns) if 'valid_concerns' in locals() else 0\n",
    "    }\n",
    "    \n",
    "    # Create reports directory if it doesn't exist\n",
    "    os.makedirs('reports', exist_ok=True)\n",
    "    \n",
    "    with open('reports/sentiment_analysis_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    return report\n",
    "\n",
    "def train_user_behavior_model(self, user_data_path):\n",
    "    \"\"\"Train model to predict user behavior\"\"\"\n",
    "    # Load survey data and transform it for modeling\n",
    "    survey_data = pd.read_csv('survey_data.csv')\n",
    "    \n",
    "    # Create derived features from survey data\n",
    "    # Convert yes/no responses to binary\n",
    "    binary_map = {'Yes': 1, 'No': 0}\n",
    "    if 'Do you follow a low-sodium diet?' in survey_data.columns:\n",
    "        survey_data['sodium_restriction_level'] = survey_data['Do you follow a low-sodium diet?'].map(binary_map).fillna(0)\n",
    "    \n",
    "    # Map awareness to technology comfort\n",
    "    if 'Are you aware of technologies that enhance taste perception without adding sodium?' in survey_data.columns:\n",
    "        survey_data['technology_comfort'] = survey_data['Are you aware of technologies that enhance taste perception without adding sodium?'].map(binary_map).fillna(0)\n",
    "    \n",
    "    # Create a binary target: device interest\n",
    "    device_interest_map = {'Yes': 1, 'No': 0, 'Maybe': 0.5}  # Consider 'Maybe' as partial interest\n",
    "    if 'Would you be interested in trying a device that enhances salty and umami flavors using electric stimulation?' in survey_data.columns:\n",
    "        survey_data['device_interest'] = survey_data['Would you be interested in trying a device that enhances salty and umami flavors using electric stimulation?'].map(device_interest_map).fillna(0)\n",
    "    \n",
    "    # Create continued_usage proxy from purchase interest\n",
    "    purchase_map = {'Yes': 1, 'Maybe': 0.5, 'No': 0}\n",
    "    if 'Would you consider purchasing such a device if it improves taste satisfaction without adding sodium?' in survey_data.columns:\n",
    "        survey_data['continued_usage'] = survey_data['Would you consider purchasing such a device if it improves taste satisfaction without adding sodium?'].map(purchase_map)\n",
    "        # Convert to binary for classification\n",
    "        survey_data['continued_usage'] = (survey_data['continued_usage'] >= 0.5).astype(int)\n",
    "    \n",
    "    # Create a taste sensitivity feature from salt preference\n",
    "    salt_preference_map = {'Too Salty': 0, 'Perfect': 0.5, 'Too Bland': 1}\n",
    "    if 'Do you find the current salt content in these dishes' in survey_data.columns:\n",
    "        survey_data['taste_sensitivity'] = survey_data['Do you find the current salt content in these dishes'].map(salt_preference_map).fillna(0.5)\n",
    "    \n",
    "    # Create previous adjustments from salt addition habits\n",
    "    adjustment_map = {'Always': 1, 'Sometimes': 0.5, 'Never': 0}\n",
    "    if 'Have you ever added salt or condiments to enhance taste in low-sodium dishes?' in survey_data.columns:\n",
    "        survey_data['previous_adjustments'] = survey_data['Have you ever added salt or condiments to enhance taste in low-sodium dishes?'].map(adjustment_map).fillna(0)\n",
    "    \n",
    "    # Create a proxy for average usage time based on restaurant frequency\n",
    "    freq_map = {'Daily': 30, 'Weekly': 20, 'Monthly': 10, 'Rarely': 5}\n",
    "    if 'How often do you eat at restaurants?' in survey_data.columns:\n",
    "        survey_data['average_usage_time'] = survey_data['How often do you eat at restaurants?'].map(freq_map).fillna(5)\n",
    "    \n",
    "    # Create gender binary feature\n",
    "    gender_map = {'Male': 1, 'Female': 0}\n",
    "    if 'Gender' in survey_data.columns:\n",
    "        survey_data['gender'] = survey_data['Gender'].str.strip().map(gender_map).fillna(0)\n",
    "    \n",
    "    # Feature engineering\n",
    "    features = [\n",
    "        'Age', 'gender', 'sodium_restriction_level', \n",
    "        'technology_comfort', 'average_usage_time',\n",
    "        'taste_sensitivity', 'previous_adjustments'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all features exist\n",
    "    available_features = []\n",
    "    for feature in features:\n",
    "        if feature in survey_data.columns:\n",
    "            available_features.append(feature)\n",
    "        else:\n",
    "            print(f\"Warning: Feature '{feature}' not found in data\")\n",
    "    \n",
    "    if not available_features or 'continued_usage' not in survey_data.columns:\n",
    "        print(\"Not enough features available for modeling\")\n",
    "        return None\n",
    "    \n",
    "    X = survey_data[available_features]\n",
    "    y = survey_data['continued_usage']  # Target: whether they would purchase the device\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(X.mean())\n",
    "    \n",
    "    # Split data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train model\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Save the model\n",
    "        model.save('models/sodium_prediction_model.pkl')\n",
    "        self.food_recognition_model = model\n",
    "    \n",
    "    # Evaluate\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.savefig('reports/confusion_matrix.png')\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': available_features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "    plt.title('Features Affecting Purchase Decision')\n",
    "    plt.savefig('reports/feature_importance.png')\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    import joblib\n",
    "    joblib.dump(model, 'models/user_behavior_model.pkl')\n",
    "    self.user_behavior_model = model\n",
    "    \n",
    "    return model, feature_importance\n",
    "\n",
    "def predict_user_retention(self, user_data):\n",
    "    \"\"\"Predict whether a user will continue using the device\"\"\"\n",
    "    if self.user_behavior_model is None:\n",
    "        print(\"User behavior model not loaded\")\n",
    "        return None\n",
    "    \n",
    "    # Ensure data is in the right format\n",
    "    if isinstance(user_data, dict):\n",
    "        user_data = pd.DataFrame([user_data])\n",
    "        # User retention prediction\n",
    "    try:\n",
    "        prediction = self.user_behavior_model.predict(user_data)\n",
    "        probability = self.user_behavior_model.predict_proba(user_data)\n",
    "        \n",
    "        return {\n",
    "            'will_continue': bool(prediction[0]),\n",
    "            'probability': float(probability[0][1] if prediction[0] else probability[0][0])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting user retention: {e}\")\n",
    "        return None\n",
    "\n",
    "# System Integration\n",
    "def process_food_image(self, image_path, user_id):\n",
    "    \"\"\"Process a food image and make recommendations\"\"\"\n",
    "    # Step 1: Food recognition\n",
    "    food_info = self.identify_food(image_path)\n",
    "    if food_info is None:\n",
    "        return {\"error\": \"Could not identify food\"}\n",
    "    \n",
    "    # Step 2: Get user preferences\n",
    "    user_preferences = self.get_user_preferences(user_id)\n",
    "    \n",
    "    # Step 3: Predict sodium content\n",
    "    sodium_content = self.predict_sodium_content(food_info['food_type'])\n",
    "    \n",
    "    # Step 4: Recommend stimulation level\n",
    "    stimulation_level = self.recommend_stimulation_level(\n",
    "        food_info['food_type'], \n",
    "        user_preferences.get('saltiness_preference', 0.5)\n",
    "    )\n",
    "    \n",
    "    # Step 5: Log the interaction\n",
    "    self.log_interaction(user_id, food_info['food_type'], stimulation_level)\n",
    "    \n",
    "    return {\n",
    "        \"food_type\": food_info['food_type'],\n",
    "        \"confidence\": food_info['confidence'],\n",
    "        \"sodium_content\": sodium_content,\n",
    "        \"recommended_stimulation_level\": stimulation_level\n",
    "    }\n",
    "\n",
    "def get_user_preferences(self, user_id):\n",
    "    \"\"\"Get user preferences from database\"\"\"\n",
    "    # In a real implementation, this would query a database\n",
    "    # For now, return default values\n",
    "    return {\n",
    "        \"saltiness_preference\": 0.5,  # Scale from 0-1\n",
    "        \"previous_adjustments\": []\n",
    "    }\n",
    "\n",
    "def log_interaction(self, user_id, food_type, stimulation_level):\n",
    "    \"\"\"Log user interaction for future analysis\"\"\"\n",
    "    # In a real implementation, this would write to a database\n",
    "    log_entry = {\n",
    "        \"user_id\": user_id,\n",
    "        \"timestamp\": pd.Timestamp.now(),\n",
    "        \"food_type\": food_type,\n",
    "        \"stimulation_level\": stimulation_level\n",
    "    }\n",
    "    \n",
    "    # For now, just print the log\n",
    "    print(f\"Logged interaction: {log_entry}\")\n",
    "\n",
    "def generate_comprehensive_report(self):\n",
    "    \"\"\"Generate a comprehensive report of all data science components\"\"\"\n",
    "    # Create a report directory if it doesn't exist\n",
    "    os.makedirs('reports', exist_ok=True)\n",
    "    \n",
    "    report = {\n",
    "        \"food_recognition\": {\n",
    "            \"model_available\": self.food_recognition_model is not None,\n",
    "            \"accuracy\": 0.85  # Placeholder\n",
    "        },\n",
    "        \"sodium_prediction\": {\n",
    "            \"model_available\": self.sodium_prediction_model is not None,\n",
    "            \"accuracy\": 0.78  # Placeholder\n",
    "        },\n",
    "        \"user_behavior\": {\n",
    "            \"model_available\": self.user_behavior_model is not None,\n",
    "            \"accuracy\": 0.82  # Placeholder\n",
    "        },\n",
    "        \"database\": {\n",
    "            \"food_database_available\": self.food_database is not None,\n",
    "            \"food_items\": len(self.food_database) if self.food_database is not None else 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save the report\n",
    "    with open('reports/comprehensive_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    return report\n",
    "\n",
    "FoodRecognitionModel.analyze_user_feedback = analyze_user_feedback\n",
    "FoodRecognitionModel.generate_comprehensive_report = generate_comprehensive_report\n",
    "FoodRecognitionModel.log_interaction = log_interaction\n",
    "FoodRecognitionModel.get_user_preferences = get_user_preferences\n",
    "FoodRecognitionModel.process_food_image = process_food_image\n",
    "FoodRecognitionModel.predict_user_retention = predict_user_retention\n",
    "FoodRecognitionModel.train_user_behavior_model = train_user_behavior_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fba764ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(self):\n",
    "        \"\"\"Load pre-trained models if available\"\"\"\n",
    "        try:\n",
    "            if os.path.exists('models/food_recognition_model.h5'):\n",
    "                self.food_recognition_model = load_model('models/food_recognition_model.h5')\n",
    "                print(\"Food recognition model loaded\")\n",
    "            \n",
    "            if os.path.exists('models/sodium_prediction_model.pkl'):\n",
    "                self.sodium_prediction_model = joblib.load('models/sodium_prediction_model.pkl')\n",
    "                print(\"Sodium prediction model loaded\")\n",
    "                \n",
    "            if os.path.exists('models/user_behavior_model.pkl'):\n",
    "                self.user_behavior_model = joblib.load('models/user_behavior_model.pkl')\n",
    "                print(\"User behavior model loaded\")\n",
    "                \n",
    "            food_database_path = r'C:\\Users\\patel\\Downloads\\UECFOOD256'\n",
    "\n",
    "            if os.path.exists(food_database_path):\n",
    "                self.food_database = pd.read_csv(food_database_path)\n",
    "                print(\"Food database loaded\")\n",
    "            else:\n",
    "                print(\"Food database not found at the specified path.\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading models: {e}\")\n",
    "\n",
    "FoodRecognitionModel.load_models=load_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45f1cb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food recognition model not loaded\n",
      "{'error': 'Could not identify food'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spoon_ds = FoodRecognitionModel()\n",
    "    \n",
    "    # Load pre-trained models\n",
    "    spoon_ds.load_models()\n",
    "    \n",
    "    # Example: Process a food image\n",
    "    result = spoon_ds.process_food_image('15847.jpg', user_id=123)\n",
    "    print(result)\n",
    "    \n",
    "#     # Example: Analyze survey data\n",
    "#     survey_report = spoon_ds.analyze_survey_data('data/survey_results.csv')\n",
    "#     print(\"Survey analysis complete\")\n",
    "    \n",
    "#     # Example: Analyze user feedback\n",
    "#     feedback_report = spoon_ds.analyze_user_feedback('data/user_feedback.csv')\n",
    "#     print(\"Feedback analysis complete\")\n",
    "    \n",
    "#     # Generate comprehensive report\n",
    "#     final_report = spoon_ds.generate_comprehensive_report()\n",
    "#     print(\"Comprehensive report generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbaad1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
